# PyTorch: Optimizasyon AlgoritmalarÄ± ve Learning Rate Scheduling Analizi

Bu proje, PyTorch kullanÄ±larak aynÄ± Sinir AÄŸÄ± mimarisinin (MLP) farklÄ± Optimizasyon AlgoritmalarÄ± (Optimizers) ve Ã–ÄŸrenme OranÄ± ZamanlayÄ±cÄ±larÄ± (LR Schedulers) ile nasÄ±l eÄŸitildiÄŸini karÅŸÄ±laÅŸtÄ±rmaktadÄ±r. 

AmaÃ§, "Hangi optimizer en hÄ±zlÄ± yakÄ±nsÄ±yor?", "LR scheduler kullanmak performansÄ± nasÄ±l etkiliyor?" gibi teorik sorulara ampirik (deneysel) kanÄ±tlar sunmaktÄ±r.

## ğŸ“Š GÃ¶rev 1: Optimizer KarÅŸÄ±laÅŸtÄ±rmasÄ±
Bu deneyde aynÄ± model; `SGD`, `SGD+Momentum`, `Adam` ve `AdamW` kullanÄ±larak eÄŸitilmiÅŸ ve Loss/Accuracy grafikleri Ã§Ä±kartÄ±lmÄ±ÅŸtÄ±r.

![Optimizer Comparison](optimizer_comparison.png)

### Soru & Cevap Analizi:
**1. Hangi optimizer en hÄ±zlÄ± yakÄ±nsÄ±yor (converge)?**

* EÄŸrilerden de gÃ¶rÃ¼lebileceÄŸi Ã¼zere **Adam ve AdamW** en hÄ±zlÄ± yakÄ±nsayan algoritmalardÄ±r. HenÃ¼z ilk epoch'larda bile loss deÄŸerleri dramatik ÅŸekilde dÃ¼ÅŸmÃ¼ÅŸ ve yÃ¼ksek doÄŸruluk oranlarÄ±na ulaÅŸÄ±lmÄ±ÅŸtÄ±r. Bunun nedeni, Adam'Ä±n her parametre iÃ§in Ã¶ÄŸrenme oranÄ±nÄ± (learning rate) gradyanlarÄ±n hareketli ortalamasÄ±na (momentum ve varyans) gÃ¶re dinamik olarak ayarlamasÄ±dÄ±r (Adaptive Learning Rate).

**2. Hangi optimizer en iyi final performansÄ± veriyor?**
* Klasik SGD Ã§ok yavaÅŸ yakÄ±nsasa da, **SGD + Momentum** genellikle son epoch'larda Adam'Ä± yakalar ve hatta bazen geÃ§er. Adam Ã§ok hÄ±zlÄ± Ã¶ÄŸrenir ancak bazen keskin (sharp) minimumlara takÄ±larak test verisinde ufak bir performans kaybÄ± (generalization gap) yaÅŸayabilir. Bu deneyde **AdamW** (Weight Decay'i daha doÄŸru uygulayan Adam varyasyonu) ve **SGD+Momentum** en yÃ¼ksek ve en stabil final test doÄŸruluklarÄ±nÄ± sunmuÅŸtur.

---

## ğŸ“‰ GÃ¶rev 2: Learning Rate Scheduling (Ã–ÄŸrenme OranÄ± Zamanlama)
Modeli sabit bir Learning Rate ile eÄŸitmek yerine, eÄŸitime yÃ¼ksek bir LR ile baÅŸlayÄ±p (hÄ±zlÄ± ilerlemek iÃ§in) minimum noktasÄ±na yaklaÅŸtÄ±kÃ§a LR'yi dÃ¼ÅŸÃ¼rmek (hedefi kaÃ§Ä±rmamak iÃ§in ince ayar yapmak) genellikle daha iyi sonuÃ§lar verir. 

Bu deneyde taban algoritma olarak SGD+Momentum (BaÅŸlangÄ±Ã§ LR=0.05) kullanÄ±lmÄ±ÅŸ ve Ã¼Ã§ farklÄ± strateji test edilmiÅŸtir.

![Scheduler Comparison](scheduler_comparison.png)

### ZamanlayÄ±cÄ±larÄ±n Karakteristikleri:

1. **Constant (No Scheduler):** EÄŸitim boyunca LR 0.05 olarak kaldÄ±. Model ilk baÅŸta hÄ±zlÄ± Ã¶ÄŸrendi ancak minimum noktasÄ± etrafÄ±nda salÄ±ndÄ±ÄŸÄ± iÃ§in (overshooting) test doÄŸruluÄŸunda dalgalanmalar yaÅŸadÄ±.
2. **StepLR:** Her 5 epoch'ta bir LR'yi %90 azalttÄ± (gamma=0.1). Grafikteki basamaklÄ± yapÄ± budur. LR dÃ¼ÅŸtÃ¼ÄŸÃ¼ anda test doÄŸruluÄŸunda anlÄ±k ve keskin bir sÄ±Ã§rama (iyileÅŸme) gÃ¶rÃ¼lÃ¼r.
3. **CosineAnnealing:** LR'yi bir kosinÃ¼s eÄŸrisi izleyerek yavaÅŸÃ§a ve pÃ¼rÃ¼zsÃ¼zce sÄ±fÄ±ra doÄŸru Ã§eker. OlasÄ± ÅŸoklarÄ± engeller.
4. **ReduceLROnPlateau:** Sadece modelin Ã¶ÄŸrenmesi durduÄŸunda (Validation Loss dÃ¼zleÅŸtiÄŸinde - plateau) LR'yi yarÄ±ya bÃ¶ler. En "akÄ±llÄ±" scheduler budur Ã§Ã¼nkÃ¼ ezbere adÄ±m atmaz, modelin performansÄ±nÄ± izler.

### Soru & Cevap Analizi:
**Hangi scheduler en iyi sonuÃ§ veriyor?**
* Grafikte aÃ§Ä±kÃ§a gÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere, sabit (Constant) bÄ±rakÄ±lan model belirli bir doÄŸruluÄŸa sÄ±kÄ±ÅŸÄ±rken, **StepLR** ve **CosineAnnealing** gibi LR'yi zamanla dÃ¼ÅŸÃ¼ren teknikler test doÄŸruluklarÄ±nda belirgin bir sÄ±Ã§rama yaratmÄ±ÅŸtÄ±r. 
* Bu veri seti iÃ§in **CosineAnnealing**, Ã¶ÄŸrenme oranÄ±nÄ± yumuÅŸak bir ÅŸekilde dÃ¼ÅŸÃ¼rdÃ¼ÄŸÃ¼ iÃ§in hem istikrarlÄ± bir Ã¶ÄŸrenme saÄŸlamÄ±ÅŸ hem de en yÃ¼ksek nihai (final) test doÄŸruluÄŸuna ulaÅŸarak en iyi sonucu vermiÅŸtir.

---

## ğŸ’» Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install torch torchvision pandas numpy matplotlib