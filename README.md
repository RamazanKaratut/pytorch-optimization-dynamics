# PyTorch: Optimizasyon AlgoritmalarÄ± ve Learning Rate Scheduling Analizi

Bu proje, PyTorch kullanÄ±larak aynÄ± Sinir AÄŸÄ± mimarisinin (MLP) farklÄ± Optimizasyon AlgoritmalarÄ± (Optimizers) ve Ã–ÄŸrenme OranÄ± ZamanlayÄ±cÄ±larÄ± (LR Schedulers) ile nasÄ±l eÄŸitildiÄŸini karÅŸÄ±laÅŸtÄ±rmaktadÄ±r. 

AmaÃ§, "Hangi optimizer en hÄ±zlÄ± yakÄ±nsÄ±yor?", "LR scheduler kullanmak performansÄ± nasÄ±l etkiliyor?" gibi teorik sorulara ampirik (deneysel) kanÄ±tlar sunmaktÄ±r.

## ğŸ“Š GÃ¶rev 1: Optimizer KarÅŸÄ±laÅŸtÄ±rmasÄ±
Bu deneyde aynÄ± model; `SGD`, `SGD+Momentum`, `Adam` ve `AdamW` kullanÄ±larak eÄŸitilmiÅŸ ve Loss/Accuracy grafikleri Ã§Ä±kartÄ±lmÄ±ÅŸtÄ±r.

![Optimizer Comparison](optimizer_comparison.png)

### Soru & Cevap Analizi:
**1. Hangi optimizer en hÄ±zlÄ± yakÄ±nsÄ±yor (converge)?**

* EÄŸrilerden ve eÄŸitim loglarÄ±ndan da net bir ÅŸekilde gÃ¶rÃ¼lebileceÄŸi Ã¼zere **Adam ve AdamW** en hÄ±zlÄ± yakÄ±nsayan algoritmalardÄ±r. HenÃ¼z 2. epoch'ta Adam'Ä±n eÄŸitim kaybÄ± (Train Loss) 0.09 seviyelerine dÃ¼ÅŸerken, ivmesiz klasik SGD 0.29 seviyelerinde kalmÄ±ÅŸtÄ±r. Bunun nedeni, Adam'Ä±n her parametre iÃ§in Ã¶ÄŸrenme oranÄ±nÄ± (learning rate) gradyanlarÄ±n hareketli ortalamasÄ±na gÃ¶re dinamik olarak ayarlamasÄ±dÄ±r (Adaptive Learning Rate).

**2. Hangi optimizer en iyi final performansÄ± veriyor?**
* Deney sonuÃ§larÄ±na gÃ¶re en yÃ¼ksek final test doÄŸruluklarÄ±na **SGD+Momentum (%98.08 peak, %98.02 final)** ve **Adam (%98.00 final)** ulaÅŸmÄ±ÅŸtÄ±r. 
* Klasik SGD ivmesi olmadÄ±ÄŸÄ± iÃ§in Ã§ok yavaÅŸ kalmÄ±ÅŸ (%96.53 final), Adam ise baÅŸta Ã§ok hÄ±zlÄ± Ã¶ÄŸrenmesine raÄŸmen son epoch'larda SGD+Momentum tarafÄ±ndan yakalanmÄ±ÅŸtÄ±r. Bu da derin Ã¶ÄŸrenmedeki o meÅŸhur *"Adam Ã§ok hÄ±zlÄ±dÄ±r ama SGD+Momentum daha iyi test performansÄ± verir (geneller)"* kuralÄ±nÄ±n pratik bir ispatÄ±dÄ±r.

---

## ğŸ“‰ GÃ¶rev 2: Learning Rate Scheduling (Ã–ÄŸrenme OranÄ± Zamanlama)
Modeli sabit bir Learning Rate ile eÄŸitmek yerine, eÄŸitime yÃ¼ksek bir LR ile baÅŸlayÄ±p minimum noktasÄ±na yaklaÅŸtÄ±kÃ§a LR'yi dÃ¼ÅŸÃ¼rmek genellikle daha iyi sonuÃ§lar verir. Bu deneyde taban algoritma olarak SGD+Momentum (BaÅŸlangÄ±Ã§ LR=0.05) kullanÄ±lmÄ±ÅŸ ve Ã¼Ã§ farklÄ± strateji test edilmiÅŸtir.

![Scheduler Comparison](scheduler_comparison.png)

### Soru & Cevap Analizi ve Karakteristikler:


**Hangi scheduler en iyi sonuÃ§ veriyor? Neden?**
* **CosineAnnealing (%98.57 Test Acc - Kazanan):** Ã–ÄŸrenme oranÄ±nÄ± bir kosinÃ¼s eÄŸrisi ÅŸeklinde yavaÅŸÃ§a ve pÃ¼rÃ¼zsÃ¼zce sÄ±fÄ±ra indirdiÄŸi iÃ§in modele en stabil Ã¶ÄŸrenme sÃ¼recini saÄŸlamÄ±ÅŸ ve %98.57 ile en yÃ¼ksek test doÄŸruluÄŸunu vermiÅŸtir.
* **StepLR (%98.41 Test Acc):** Her 5 epoch'ta bir LR'yi %10'una dÃ¼ÅŸÃ¼rdÃ¼. Loglara baktÄ±ÄŸÄ±mÄ±zda 5. epoch'ta %97.37 olan baÅŸarÄ±nÄ±n, LR dÃ¼ÅŸtÃ¼kten hemen sonra 6. epoch'ta aniden **%98.27**'ye sÄ±Ã§radÄ±ÄŸÄ± gÃ¶rÃ¼lmektedir. Ancak ani ÅŸoklar nedeniyle CosineAnnealing'in biraz gerisinde kalmÄ±ÅŸtÄ±r.
* **ReduceLROnPlateau (%98.30 Test Acc):** Model 7. epoch'a kadar plato (dÃ¼zlÃ¼k) yapmÄ±ÅŸ, ancak 8. epoch'ta scheduler'Ä±n LR'yi dÃ¼ÅŸÃ¼rmesiyle (0.05 -> 0.025) test baÅŸarÄ±sÄ± bir anda 97.37'den 98.19'a fÄ±rlamÄ±ÅŸtÄ±r.
* **Constant (No Scheduler):** EÄŸitim boyunca LR sabit (0.05) kaldÄ±ÄŸÄ± iÃ§in minimum noktasÄ± etrafÄ±nda sÃ¼rekli salÄ±nÄ±m yapmÄ±ÅŸ (overshooting) ve %98.10 ile genel olarak en dÃ¼ÅŸÃ¼k performansta kalmÄ±ÅŸtÄ±r.

---

## ğŸ’» Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:
```bash
pip install torch torchvision pandas numpy matplotlib